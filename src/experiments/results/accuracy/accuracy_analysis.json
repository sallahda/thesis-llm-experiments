{
  "overall": {
    "mean_accuracy": 4.25,
    "std_accuracy": 0.9373214115613857,
    "high_accuracy_percentage": 83.33333333333334,
    "best_score": "5",
    "worst_score": "2",
    "total_valid_evaluations": 36,
    "total_failed_evaluations": 0
  },
  "by_model": "               mean   std  count\nmodel                           \nclaude_sonnet  4.42  0.67     12\nllama_3b       3.67  1.23     12\nnova_pro       4.67  0.49     12",
  "by_prompt": "                  mean   std  count\nprompt_strategy                    \nchain_of_thought  4.22  1.09      9\nfew_shot          4.44  0.73      9\ngeneral           4.44  1.01      9\nrole_based        3.89  0.93      9",
  "by_visualization": "               mean   std  count\nvisualization                   \nheatmap        3.92  1.00     12\nstacked_bar    4.08  1.00     12\ntreemap        4.75  0.62     12",
  "best_combinations": "            model   prompt_strategy visualization  accuracy_score                                 accuracy_reasoning\n2   claude_sonnet           general       treemap               5  The narrative accurately interprets the data v...\n5   claude_sonnet        role_based       treemap               5  The narrative accurately interprets the visual...\n7   claude_sonnet          few_shot       heatmap               5  The narrative accurately interprets the data v...\n8   claude_sonnet          few_shot       treemap               5  The narrative accurately interprets the data v...\n10  claude_sonnet  chain_of_thought       heatmap               5  The narrative accurately interprets the data v...",
  "anova": {
    "model_f_stat": 4.422680412371134,
    "model_p_value": 0.019874640267796104,
    "model_significant": "True",
    "prompt_f_stat": 0.6871794871794872,
    "prompt_p_value": 0.5665108806159946,
    "prompt_significant": "False"
  }
}